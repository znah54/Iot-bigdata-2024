{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파이토치 기본\n",
    "\n",
    "- 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cu121'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 파이토치 버전확인\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이토치 패키지(네임스페이스) 구성\n",
    "- **torch**\n",
    "    - 메인 네임스페이스. 텐서(진짜 기본) 및 수학함수(sin,cos,tan...) 포함. Numpy와 유사한 구조\n",
    "    - Numpy와 쉽게 변환\n",
    "\n",
    "- **torch.autograd**\n",
    "    - 자동 미분(!)을 위한 함수 포함. 컨텍스트 매니저, 기반클래스 및 함수 포함\n",
    "\n",
    "- **torch.nn / torch.nn.functional**\n",
    "    - nn(Neural Network). 딥러닝 신경망을 구축시 필요한 데이터구조, 레이어 정의가 포함. RNN, LSTM 레이어 및 ReLU, MESLoss 등 Tensorflow에 있는 함수 및 모델 포함\n",
    "\n",
    "- **torch.optim**\n",
    "    - 확률적 경사 하강법(Stochastic Gradient Descent, SGD) 중심 파라미터 최적화 알고리즘 포함\n",
    "\n",
    "- **torch.utils.data**\n",
    "    - SGD 반복 연산 시 Batch(컴퓨터가 자동 백그라운드로 실행) 유틸리티가 포함\n",
    "\n",
    "- **torch.onnx**\n",
    "    - Open Neural Network eXchange 포맷의 모델 포함. onnx 확장자로 export하면 Tensorflow나 다른 딥러닝,머신러닝 라이브러리에서 사용 기능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이토치 기초\n",
    "\n",
    "##### 텐서(Tensor)\n",
    "- 파이토치에서 데이터를 저장하는 자료구조\n",
    "- 텐서플로도 기본 텐서, Numpy와 성격과 사용법이 흡사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1차원 크기3 배열을 넘파이로\n",
    "data = [1, 3, 5]\n",
    "np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1차원 크기3의 배열을 텐서로\n",
    "torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [5, 6, 7, 8]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 파이썬 2차원 리스트\n",
    "arr = [[1,2,3,4], [5,6,7,8]]\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2차원 2행4열 넘파이\n",
    "np2_data = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2차원 2행4열 텐서\n",
    "tc2_data = torch.tensor(arr)\n",
    "tc2_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 텐서의 구조 명칭\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/814efc38b1d8ae9cf47a24e12c8aae325fb3ab56e04c601d98c967d2a182e758/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6875676f4d4753756e672f73747564792d7079746f7263682f6d61696e2f696d616765732f746f726368303030332e706e67\" width=\"730\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1차원 텐서 -> 벡터(Vector)\n",
    "- 2차원 텐서 -> 매트릭스(Matrix)\n",
    "- 3차원이상 텐서 -> 텐서(Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4) torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "## 넘파이와 토치의 배열크기\n",
    "print(np2_data.shape, tc2_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32 torch.int64\n"
     ]
    }
   ],
   "source": [
    "## 넘파이는 정수의 기본타입이 int32, 파이토치는 정수 기본타입이 int64\n",
    "print(np2_data.dtype, tc2_data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 텐서를 만들때 타입을 지정해서 변경가능\n",
    "torch.tensor(data=arr, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 텐서의 데이터타입\n",
    "- 실수 계산 시 FloatTensor, 정수를 사용할 때는 LongTensor, True/False는 ByteTensor 사용\n",
    "- 객체클래스 - 데이터타입\n",
    "- ByteTensor, CharTensor - int8\n",
    "- ShortTensor - int16\n",
    "- IntTensor - int32\n",
    "- LongTensor - int64\n",
    "- HalfTensor - float16\n",
    "- FloatTensor - float32\n",
    "- DoubleTensor - float64\n",
    "- ...\n",
    "- 참조 : https://subinium.github.io/pytorch-Tensor-Variable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 8.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 테이터타입으로 변환시, 31셀과 동일한 결과\n",
    "tc2_data.type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 텐서 구조,상태 확인\n",
    "- tensor.shape, tensor,.size() - 텐서의 크기\n",
    "- tensor.dtype, tensor.type() - 텐서의 데이터타입 확인\n",
    "    - dtype 데이터 타입, type() 객체의 클래스 타입\n",
    "- tensor.ndim, tensor.dim() - 텐서의 차원\n",
    "- tensor.numel() - 전체 원소 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7, 9])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터\n",
    "tns1_data = torch.tensor([1,3,5,7,9])\n",
    "tns1_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 매트릭스\n",
    "tns2_data = torch.tensor([[1,2,3,4],[5,6,7,8]])\n",
    "tns2_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5],\n",
       "         [ 6,  7,  8]],\n",
       "\n",
       "        [[ 9, 10, 11],\n",
       "         [12, 13, 14],\n",
       "         [15, 16, 17]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23],\n",
       "         [24, 25, 26]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 넘파이로 3차원 생성, 텐서\n",
    "tns3_data = torch.tensor(np.arange(27).reshape(3, 3, 3))\n",
    "tns3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector num = 5 \n",
      "Matrix num = 8 \n",
      "Tensor num = 27\n"
     ]
    }
   ],
   "source": [
    "## 원소개수\n",
    "print(f'Vector num = {tns1_data.numel()} \\nMatrix num = {tns2_data.numel()} \\nTensor num = {tns3_data.numel()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 차원확인\n",
    "tns1_data.ndim, tns2_data.ndim, tns3_data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 위와 동일\n",
    "tns1_data.dim(), tns2_data.dim(), tns3_data.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.int64, torch.int64, torch.int32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 데이터 타입확인\n",
    "tns1_data.dtype, tns2_data.dtype, tns3_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('torch.LongTensor', 'torch.LongTensor', 'torch.IntTensor')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 클래스 객체 타입확인\n",
    "tns1_data.type(), tns2_data.type(), tns3_data.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5]), torch.Size([2, 4]), torch.Size([3, 3, 3]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 데이터크기\n",
    "tns1_data.shape, tns2_data.shape, tns3_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 넘파이와 파이토치 텐서간 변환 쉬움\n",
    "- 넘파이 함수와 동일한 함수 존재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## zeros\n",
    "torch.zeros(2, 4), torch.ones(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]),\n",
       " tensor([[[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.],\n",
       "          [1., 1., 1.],\n",
       "          [1., 1., 1.]]]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(3, 3, 3), torch.ones(3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 파이썬 리스트의 인덱싱, 슬라이싱 전부 가능\n",
    "tns2_data[0][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  4,  6],\n",
       "        [ 8, 10, 12]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 행렬합\n",
    "x1 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "x2 = torch.tensor([[1,2,3],[4,5,6]])\n",
    "torch.add(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [16, 25, 36]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 행렬곱\n",
    "torch.mul(x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22, 28],\n",
       "        [49, 64]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 매트릭스연산, 첫번째 매트릭스 열크기와, 두번째 매트릭스 행크기가 일치\n",
    "x3 = torch.tensor([[1,2],[3,4],[5,6]])\n",
    "torch.mm(x1, x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CPU 메모리에서 GPU 메모리도 데이터 전달\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 현재의 데이터가 어디 메모리에 있는지\n",
    "x3.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CPU 디바이스 = 'cpu'\n",
    "cpu = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU 디바이스 = 'cuda'\n",
    "gpu = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]], device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4 = x3.to(gpu)\n",
    "x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x5 = x4.to(cpu)\n",
    "x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아직도 텐서와 기본 함수가 엄청 많음. 공부 많이해야 함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
